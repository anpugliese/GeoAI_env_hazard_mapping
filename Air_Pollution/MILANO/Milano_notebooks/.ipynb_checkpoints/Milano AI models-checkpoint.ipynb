{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c17fbcbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../../..\")\n",
    "\n",
    "from datetime import date\n",
    "import pandas as pd\n",
    "import plotly.graph_objects as go\n",
    "import numpy as np\n",
    "import rasterio as rio\n",
    "import rioxarray as rxr\n",
    "\n",
    "\n",
    "from modules import processing_module as processing\n",
    "from modules import interpolation_module as interp\n",
    "from modules import ai_module as ai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b7763fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(pollutant, training_mode, prefix):\n",
    "    train_path = f'../harmonia_processor/{pollutant}/train/training_{training_mode}.csv'\n",
    "    training_dataset = importer.import_df(train_path, date_format='%Y-%m-%d')\n",
    "    training_dataset = training_dataset.dropna()\n",
    "    training_dates = training_dataset.copy()[['date']]\n",
    "    if 'date' in list(training_dataset.columns):\n",
    "        training_dataset = training_dataset.drop(['date'], axis=1)\n",
    "\n",
    "    test_path = f'../harmonia_processor/{pollutant}/train/validation_{training_mode}.csv'\n",
    "    testing_dataset = importer.import_df(test_path, date_format='%Y-%m-%d')\n",
    "    testing_dataset = testing_dataset.dropna()\n",
    "    testing_dates = testing_dataset.copy()[['date']]\n",
    "    if 'date' in list(testing_dataset.columns):\n",
    "        testing_dataset = testing_dataset.drop(['date'], axis=1)\n",
    "\n",
    "    #save the training columns for selecting them in the prediction dataset\n",
    "    training_columns = list(training_dataset.columns)\n",
    "    training_columns.remove('exc')\n",
    "\n",
    "    model_path = f'../harmonia_processor/{pollutant}/model/{prefix}_model_{training_mode}.csv'\n",
    "    ai_model = ai.MLProcessor(training_dataset.copy(), testing_dataset.copy())\n",
    "    ai_model.load_model(model_path)\n",
    "    ai_model.model_type = model_to_use\n",
    "    return ai_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60cb98ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_options = {\n",
    "    'rf': {\n",
    "        \"prefix\": 'rf',\n",
    "        \"training_options\": {\n",
    "            \"normalized\": False,\n",
    "            \"n_estimators\": 500,\n",
    "            \"n_jobs\": -1,\n",
    "            \"max_depth\": 30,\n",
    "            \"random_state\": None\n",
    "        },\n",
    "        \"prediction_options\": {\n",
    "            \"normalized\": False\n",
    "        }\n",
    "    },\n",
    "    \n",
    "    'svm': {\n",
    "        \"prefix\": 'svm',\n",
    "        \"training_options\": {\n",
    "            \"normalized\": True,\n",
    "            \"kernel\": 'rbf',\n",
    "            \"probability\": True,\n",
    "            \"verbose\": False,\n",
    "            \"max_iter\": 10,\n",
    "            \"random_state\": None,\n",
    "            \"cache_size\": 1024,\n",
    "            \"n_jobs\": 5\n",
    "        },\n",
    "        \"prediction_options\":{\n",
    "            \"normalized\": True,\n",
    "        }\n",
    "    },\n",
    "    \n",
    "    'lstm': {\n",
    "        \"prefix\": 'lstm',\n",
    "        \"training_options\": {\n",
    "            \"normalized\": True,\n",
    "            \"activation\": 'sigmoid',\n",
    "            \"metrics\": ['accuracy'],\n",
    "            \"optimizer\": 'adam',\n",
    "            \"loss\": 'binary_crossentropy'\n",
    "        },\n",
    "        \"prediction_options\":{\n",
    "            \"normalized\": True,\n",
    "        }\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dc38918",
   "metadata": {},
   "outputs": [],
   "source": [
    "importer = processing.HarmoniaProcessor()\n",
    "pollutants = ['pm10', 'pm25', 'so2', 'o3']\n",
    "#Missing no2 because there are not exceedances\n",
    "models = ['rf', 'lstm']\n",
    "#models = ['rf']\n",
    "train_modes = [\n",
    "    'rand_balance',\n",
    "    'NOrand_balance',\n",
    "    'rand_NObalance',\n",
    "    'NOrand_NObalance'\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5f9a823",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Train the models\n",
    "for model_to_use in models:\n",
    "    prefix = model_options[model_to_use]['prefix']\n",
    "    print(f\"FOR {model_to_use}\")\n",
    "\n",
    "    for pollutant in pollutants:\n",
    "        print(f\"FOR {pollutant}\")\n",
    "        \n",
    "        for training_mode in train_modes:\n",
    "            try:\n",
    "                train_path = f'../harmonia_processor/{pollutant}/train/training_{training_mode}.csv'\n",
    "                training_dataset = importer.import_df(train_path, date_format='%Y-%m-%d')\n",
    "                training_dataset = training_dataset.dropna()\n",
    "                training_dates = training_dataset.copy()[['date']]\n",
    "                if 'date' in list(training_dataset.columns):\n",
    "                    training_dataset = training_dataset.drop(['date'], axis=1)\n",
    "\n",
    "                test_path = f'../harmonia_processor/{pollutant}/train/validation_{training_mode}.csv'\n",
    "                testing_dataset = importer.import_df(test_path, date_format='%Y-%m-%d')\n",
    "                testing_dataset = testing_dataset.dropna()\n",
    "                testing_dates = testing_dataset.copy()[['date']]\n",
    "                if 'date' in list(testing_dataset.columns):\n",
    "                    testing_dataset = testing_dataset.drop(['date'], axis=1)\n",
    "\n",
    "                #save the training columns for selecting them in the prediction dataset\n",
    "                training_columns = list(training_dataset.columns)\n",
    "                training_columns.remove('exc')\n",
    "\n",
    "                ai_model = ai.MLProcessor(training_dataset.copy(), testing_dataset.copy())\n",
    "\n",
    "                ai_model.train_model(\n",
    "                    model_to_use,\n",
    "                    'exc',\n",
    "                    model_options=model_options[model_to_use]['training_options']\n",
    "                )\n",
    "\n",
    "                model_path = f'../harmonia_processor/{pollutant}/model/{prefix}_model_{training_mode}.csv'\n",
    "                ai_model.save_model(model_path)\n",
    "            except Exception as ex:\n",
    "                print(f\"could not train for {pollutant}!\")\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c503e8a8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Score the models\n",
    "scores_df_columns = [\"type\", \"train_mode\", \"pollutant\", \"prefix\", \"score\"]\n",
    "scores_df = pd.DataFrame(columns=scores_df_columns)\n",
    "\n",
    "for model_to_use in models:\n",
    "    prefix = model_options[model_to_use]['prefix']\n",
    "    print(f\"FOR {model_to_use}\")\n",
    "\n",
    "    for pollutant in pollutants:\n",
    "        print(f\"FOR {pollutant}\")\n",
    "        \n",
    "        for training_mode in train_modes:\n",
    "    \n",
    "            train_path = f'../harmonia_processor/{pollutant}/train/training_{training_mode}.csv'\n",
    "            training_dataset = importer.import_df(train_path, date_format='%Y-%m-%d')\n",
    "            training_dataset = training_dataset.dropna()\n",
    "            training_dates = training_dataset.copy()[['date']]\n",
    "            if 'date' in list(training_dataset.columns):\n",
    "                training_dataset = training_dataset.drop(['date'], axis=1)\n",
    "\n",
    "            test_path = f'../harmonia_processor/{pollutant}/train/validation_{training_mode}.csv'\n",
    "            testing_dataset = importer.import_df(test_path, date_format='%Y-%m-%d')\n",
    "            testing_dataset = testing_dataset.dropna()\n",
    "            testing_dates = testing_dataset.copy()[['date']]\n",
    "            if 'date' in list(testing_dataset.columns):\n",
    "                testing_dataset = testing_dataset.drop(['date'], axis=1)\n",
    "\n",
    "            #save the training columns for selecting them in the prediction dataset\n",
    "            training_columns = list(training_dataset.columns)\n",
    "            training_columns.remove('exc')\n",
    "            \n",
    "            model_path = f'../harmonia_processor/{pollutant}/model/{prefix}_model_{training_mode}.csv'\n",
    "            ai_model = ai.MLProcessor(training_dataset.copy(), testing_dataset.copy())\n",
    "            ai_model.load_model(model_path)\n",
    "            ai_model.model_type = model_to_use\n",
    "        \n",
    "            score = ai_model.score_model()\n",
    "            score_row = [model_to_use, training_mode, pollutant, prefix, score]\n",
    "            scores_df = pd.concat([\n",
    "                scores_df, \n",
    "                pd.DataFrame(\n",
    "                    [score_row], \n",
    "                    columns=scores_df_columns\n",
    "                )\n",
    "            ])\n",
    "    \n",
    "scores_path = f'../harmonia_processor/model_scores.csv'\n",
    "scores_df.to_csv(scores_path)\n",
    "scores_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f9b7c03",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#plot for model accuracies\n",
    "for model in models:\n",
    "    plot_dfs = []\n",
    "    plot_names = []\n",
    "    sort_by = 'train_mode'\n",
    "    print(f\"--------------------------------------------------\")\n",
    "    print(f\"scores for model {model}\")\n",
    "    for pollutant in pollutants:\n",
    "        temp_plot_df = scores_df.loc[\n",
    "            (scores_df['pollutant'] == pollutant) & (scores_df['type'] == model)\n",
    "        ].reset_index(drop=True).sort_values(by=sort_by)\n",
    "        plot_dfs.append(temp_plot_df.copy())\n",
    "        plot_names.append(pollutant)\n",
    "\n",
    "    importer.show_plot(\n",
    "        plot_dfs,\n",
    "        ['train_mode', 'train_mode', 'train_mode', 'train_mode'],\n",
    "        ['score', 'score', 'score', 'score'],\n",
    "        plot_names\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ab98d46",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#calculate best models\n",
    "scores_path = f'../harmonia_processor/model_scores.csv'\n",
    "scores_df = importer.import_df(scores_path, date_format=None)\n",
    "scores_df = scores_df.reset_index(drop=True)\n",
    "\n",
    "best_model_data = {}\n",
    "for pollutant in pollutants:\n",
    "    best = scores_df.sort_values(by='score', ascending=False).loc[\n",
    "        scores_df['pollutant'] == pollutant\n",
    "    ].reset_index(drop=True).iloc[0]\n",
    "    best_path = f'../harmonia_processor/{pollutant}/model/{best.prefix}_model_{best.train_mode}.csv'\n",
    "    best_type = best.type\n",
    "    best_model_data[pollutant] = {\n",
    "        \"model_path\": best_path,\n",
    "        \"prefix\": best.prefix,\n",
    "        \"train_mode\": best.train_mode,\n",
    "        \"type\": best_type,\n",
    "        \"score\": best.score\n",
    "    }\n",
    "best_model_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c1f3e78",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#predict the stations and grid samples for each model, each month, each pollutant, for the best model score\n",
    "best_base_path_predictions = f'../best_model/predictions'\n",
    "for pollutant in pollutants:\n",
    "    print(f\"FOR {pollutant}\")\n",
    "    model_data = best_model_data[pollutant]\n",
    "    train_mode = model_data['train_mode']\n",
    "    prefix = model_data['prefix']\n",
    "    model_to_use = model_data['type']\n",
    "    \n",
    "    train_path = f'../harmonia_processor/{pollutant}/train/training_{train_mode}.csv'\n",
    "    training_dataset = importer.import_df(train_path, date_format='%Y-%m-%d')\n",
    "    training_dataset = training_dataset.dropna()\n",
    "    training_dates = training_dataset.copy()[['date']]\n",
    "    if 'date' in list(training_dataset.columns):\n",
    "        training_dataset = training_dataset.drop(['date'], axis=1)\n",
    "\n",
    "    test_path = f'../harmonia_processor/{pollutant}/train/validation_{train_mode}.csv'\n",
    "    testing_dataset = importer.import_df(test_path, date_format='%Y-%m-%d')\n",
    "    testing_dataset = testing_dataset.dropna()\n",
    "    testing_dates = testing_dataset.copy()[['date']]\n",
    "    if 'date' in list(testing_dataset.columns):\n",
    "        testing_dataset = testing_dataset.drop(['date'], axis=1)\n",
    "\n",
    "    #save the training columns for selecting them in the prediction dataset\n",
    "    training_columns = list(training_dataset.columns)\n",
    "    if 'exc' in training_columns:\n",
    "        training_columns.remove('exc')\n",
    "\n",
    "    model_path = f'../harmonia_processor/{pollutant}/model/{prefix}_model_{train_mode}.csv'\n",
    "    ai_model = ai.MLProcessor(training_dataset.copy(), testing_dataset.copy())\n",
    "    ai_model.load_model(model_path)\n",
    "    ai_model.model_type = model_to_use\n",
    "\n",
    "    predictions = {}\n",
    "    predictions_grid = {}\n",
    "    predicts = {}\n",
    "    predicts_grid = {}\n",
    "\n",
    "    for m in range(1,13):\n",
    "        print(f'FOR {pollutant} MONTH {m}')\n",
    "        print(f\"Predicting in stations datasets\")\n",
    "        predict_path = f'../harmonia_processor/{pollutant}/predict/stations_month_{m}.csv'\n",
    "        prediction_df =  importer.import_df(predict_path, date_format=None)\n",
    "        prediction_dataset = prediction_df.copy()\n",
    "        prediction_dataset = prediction_dataset[training_columns]\n",
    "        predicts[m] = prediction_df.copy()\n",
    "        if 'date' in list(prediction_dataset.columns):\n",
    "            prediction_dataset = prediction_dataset.drop(['date'], axis=1)\n",
    "\n",
    "        prediction_dataset = prediction_dataset.reset_index(drop=True).dropna()\n",
    "        if model_to_use == 'lstm':\n",
    "            predicted_probabilities = ai_model.predict(\n",
    "                prediction_dataset.copy(), \n",
    "                predict_options=model_options[model_to_use]['prediction_options']\n",
    "            )\n",
    "        else:\n",
    "            predicted_probabilities = ai_model.predict_probabilities(\n",
    "                prediction_dataset.copy(),\n",
    "                predict_options=model_options[model_to_use]['prediction_options']\n",
    "            )\n",
    "            predicted_probabilities = predicted_probabilities[:,1]\n",
    "\n",
    "        #Create a DF from predicted labels\n",
    "        predicted_df = pd.DataFrame(predicted_probabilities)\n",
    "        predicted_df.columns = ['exc']\n",
    "\n",
    "        #Concat DF with UTM coordinates \n",
    "        predicted_df = predicted_df.reset_index(drop=True)\n",
    "        prediction_locations = prediction_dataset[['lat','lng']].reset_index(drop=True)\n",
    "        predicted_df = pd.concat([predicted_df, prediction_locations],axis=1)\n",
    "        predicted_df['exc'] = predicted_df['exc']*100\n",
    "\n",
    "        predictions[m] = predicted_df.copy()\n",
    "\n",
    "        prediction_path = f'{best_base_path_predictions}/best_{pollutant}_stations_month_{m}.csv'\n",
    "        predictions[m].to_csv(prediction_path)\n",
    "\n",
    "\n",
    "        print(f\"Predicting in grid datasets\")\n",
    "        predict_path = f'../harmonia_processor/{pollutant}/predict/grid_month_{m}.csv'\n",
    "        prediction_df =  importer.import_df(predict_path, date_format=None)\n",
    "        prediction_dataset = prediction_df.copy()\n",
    "        prediction_dataset = prediction_dataset[training_columns]\n",
    "        predicts_grid[m] = prediction_df.copy()\n",
    "        if 'date' in list(prediction_dataset.columns):\n",
    "            prediction_dataset = prediction_dataset.drop(['date'], axis=1)\n",
    "\n",
    "\n",
    "        prediction_dataset = prediction_dataset.reset_index(drop=True).dropna()\n",
    "        if model_to_use == 'lstm':\n",
    "            predicted_probabilities = ai_model.predict(\n",
    "                prediction_dataset.copy(), \n",
    "                predict_options=model_options[model_to_use]['prediction_options']\n",
    "            )\n",
    "        else:\n",
    "            predicted_probabilities = ai_model.predict_probabilities(\n",
    "                prediction_dataset.copy(), \n",
    "                predict_options=model_options[model_to_use]['prediction_options']\n",
    "            )\n",
    "            predicted_probabilities = predicted_probabilities[:,1]\n",
    "\n",
    "        #Create a DF from predicted labels\n",
    "        predicted_df = pd.DataFrame(predicted_probabilities)\n",
    "        predicted_df.columns = ['exc']\n",
    "\n",
    "        #Concat DF with UTM coordinates \n",
    "        #reset index to avoid indexing problems\n",
    "        predicted_df = predicted_df.reset_index(drop=True)\n",
    "        prediction_locations = prediction_dataset[['lat','lng']].reset_index(drop=True)\n",
    "        predicted_df = pd.concat([predicted_df, prediction_locations],axis=1)\n",
    "        predicted_df['exc'] = predicted_df['exc']*100\n",
    "\n",
    "        predictions_grid[m] = predicted_df.copy()\n",
    "\n",
    "        prediction_path = f'{best_base_path_predictions}/best_{pollutant}_grid_month_{m}.csv'\n",
    "        predictions_grid[m].to_csv(prediction_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86a3fe5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "milano_shapefile = '../data/milano_final_shapefile/milano_metro.shp'\n",
    "milano_epsg = 32632\n",
    "grid = interp.create_grid_from_shapefile(milano_shapefile, xdelta=1000, ydelta=1000, shapefile_epsg=milano_epsg)\n",
    "pollutants = ['pm10', 'pm25', 'so2', 'o3']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7f03f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9de2e840",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Generate rasters with stations data\n",
    "best_raster_base_path = f'../best_model/rasters'\n",
    "for pollutant in pollutants:\n",
    "    for m in range(1,13):\n",
    "        prediction_path = f'{best_base_path_predictions}/best_{pollutant}_stations_month_{m}.csv'\n",
    "        prediction_m =  importer.import_df(prediction_path, date_format=None)\n",
    "        print(f\"------ {pollutant} --- month {m} ------\")\n",
    "        interpolated_to_grid,b = interp.interpolate(\n",
    "            'exc', \n",
    "            'NN', \n",
    "            milano_shapefile,\n",
    "            prediction_m.copy(), \n",
    "            visual_output=True,\n",
    "            epsg_utm=milano_epsg\n",
    "        )\n",
    "\n",
    "        new_interp = pd.DataFrame()\n",
    "        new_interp['y'] = interpolated_to_grid.original_centroids.y\n",
    "        new_interp['x'] = interpolated_to_grid.original_centroids.x\n",
    "        new_interp['value'] = interpolated_to_grid.NN\n",
    "        interp_xar = new_interp.set_index(['y', 'x']).to_xarray()\n",
    "\n",
    "        raster_path = f'{best_raster_base_path}/stations_{pollutant}_month_{m}.tiff'\n",
    "        \n",
    "        array_to_write = np.flip(np.flip(interp_xar.value.to_numpy()), axis=1)\n",
    "        profile = {\n",
    "            'driver': 'GTiff', \n",
    "            'dtype': 'float32', \n",
    "            'nodata': -9999.0, \n",
    "            'width': int(len(interp_xar.x)), \n",
    "            'height': int(len(interp_xar.y)), \n",
    "            'count': 1, \n",
    "            'crs': rio.CRS.from_epsg(32632), \n",
    "            'transform': rio.Affine(1000.0, 0.0, int(interp_xar.x[0]), 0.0, -1000.0, int(interp_xar.y[-1])), \n",
    "            'tiled': False, \n",
    "            'interleave': 'band'\n",
    "        }\n",
    "        with rio.open(raster_path, 'w', **profile) as dest:\n",
    "            dest.write(array_to_write, 1)\n",
    "    \n",
    "        #prediction_raster = interp.save_as_raster(new_interp, raster_path, crs=milano_epsg)\n",
    "        print(f\"saved raster to {raster_path}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fb2b49e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Generate rasters with grid data\n",
    "best_raster_base_path = f'../best_model/rasters'\n",
    "for pollutant in pollutants:\n",
    "    for m in range(1,13):\n",
    "        prediction_path = f'{best_base_path_predictions}/best_{pollutant}_grid_month_{m}.csv'\n",
    "        prediction_m =  importer.import_df(prediction_path, date_format=None)\n",
    "        print(f\"------ {pollutant} --- month {m} ------\")\n",
    "\n",
    "        interpolated_to_grid,b = interp.interpolate(\n",
    "            'exc', \n",
    "            'NN', \n",
    "            milano_shapefile, \n",
    "            prediction_m, \n",
    "            visual_output=True,\n",
    "            epsg_utm=milano_epsg\n",
    "        )\n",
    "\n",
    "        new_interp = pd.DataFrame()\n",
    "        new_interp['y'] = interpolated_to_grid.original_centroids.y\n",
    "        new_interp['x'] = interpolated_to_grid.original_centroids.x\n",
    "        new_interp['value'] = interpolated_to_grid.NN\n",
    "        \n",
    "        interp_xar = new_interp.set_index(['y', 'x']).to_xarray()\n",
    "\n",
    "        raster_path = f'{best_raster_base_path}/grid_{pollutant}_month_{m}.tiff'\n",
    "        \n",
    "        array_to_write = np.flip(np.flip(interp_xar.value.to_numpy()), axis=1)\n",
    "        profile = {\n",
    "            'driver': 'GTiff', \n",
    "            'dtype': 'float32', \n",
    "            'nodata': -9999.0, \n",
    "            'width': int(len(interp_xar.x)), \n",
    "            'height': int(len(interp_xar.y)), \n",
    "            'count': 1, \n",
    "            'crs': rio.CRS.from_epsg(32632), \n",
    "            'transform': rio.Affine(1000.0, 0.0, int(interp_xar.x[0]), 0.0, -1000.0, int(interp_xar.y[-1])), \n",
    "            'tiled': False, \n",
    "            'interleave': 'band'\n",
    "        }\n",
    "        with rio.open(raster_path, 'w', **profile) as dest:\n",
    "            dest.write(array_to_write, 1)\n",
    "\n",
    "        \n",
    "        #prediction_raster = interp.save_as_raster(new_interp, raster_path, crs=milano_epsg)\n",
    "        print(f\"saved raster to {raster_path}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c431f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#PREDICT ALL!\n",
    "best_base_path_predictions = f'../harmonia_processor'\n",
    "for pollutant in pollutants:\n",
    "    print(f\"FOR {pollutant}\")\n",
    "    for model_to_use in models:\n",
    "        print(f\"FOR {model_to_use}\")\n",
    "        for train_mode in train_modes:\n",
    "            print(f\"FOR {train_mode}\")\n",
    "            prefix = model_options[model_to_use]['prefix']\n",
    "\n",
    "            train_path = f'{best_base_path_predictions}/{pollutant}/train/training_{train_mode}.csv'\n",
    "            training_dataset = importer.import_df(train_path, date_format='%Y-%m-%d')\n",
    "            training_dataset = training_dataset.dropna()\n",
    "            training_dates = training_dataset.copy()[['date']]\n",
    "            if 'date' in list(training_dataset.columns):\n",
    "                training_dataset = training_dataset.drop(['date'], axis=1)\n",
    "\n",
    "            test_path = f'{best_base_path_predictions}/{pollutant}/train/validation_{train_mode}.csv'\n",
    "            testing_dataset = importer.import_df(test_path, date_format='%Y-%m-%d')\n",
    "            testing_dataset = testing_dataset.dropna()\n",
    "            testing_dates = testing_dataset.copy()[['date']]\n",
    "            if 'date' in list(testing_dataset.columns):\n",
    "                testing_dataset = testing_dataset.drop(['date'], axis=1)\n",
    "\n",
    "            #save the training columns for selecting them in the prediction dataset\n",
    "            training_columns = list(training_dataset.columns)\n",
    "            if 'exc' in training_columns:\n",
    "                training_columns.remove('exc')\n",
    "\n",
    "            model_path = f'{best_base_path_predictions}/{pollutant}/model/{prefix}_model_{train_mode}.csv'\n",
    "            ai_model = ai.MLProcessor(training_dataset.copy(), testing_dataset.copy())\n",
    "            ai_model.load_model(model_path)\n",
    "            ai_model.model_type = model_to_use\n",
    "\n",
    "            predictions = {}\n",
    "            predictions_grid = {}\n",
    "            predicts = {}\n",
    "            predicts_grid = {}\n",
    "\n",
    "            for m in range(1,13):\n",
    "                print(f'FOR {pollutant} MONTH {m}')\n",
    "                print(f\"Predicting in stations datasets\")\n",
    "                predict_path = f'{best_base_path_predictions}/{pollutant}/predict/stations_month_{m}.csv'\n",
    "                prediction_df =  importer.import_df(predict_path, date_format=None)\n",
    "                prediction_dataset = prediction_df.copy()\n",
    "                prediction_dataset = prediction_dataset[training_columns]\n",
    "                predicts[m] = prediction_df.copy()\n",
    "                if 'date' in list(prediction_dataset.columns):\n",
    "                    prediction_dataset = prediction_dataset.drop(['date'], axis=1)\n",
    "\n",
    "                prediction_dataset = prediction_dataset.reset_index(drop=True).dropna()\n",
    "                if model_to_use == 'lstm':\n",
    "                    predicted_probabilities = ai_model.predict(\n",
    "                        prediction_dataset.copy(), \n",
    "                        predict_options=model_options[model_to_use]['prediction_options']\n",
    "                    )\n",
    "                else:\n",
    "                    predicted_probabilities = ai_model.predict_probabilities(\n",
    "                        prediction_dataset.copy(),\n",
    "                        predict_options=model_options[model_to_use]['prediction_options']\n",
    "                    )\n",
    "                    predicted_probabilities = predicted_probabilities[:,1]\n",
    "\n",
    "                #Create a DF from predicted labels\n",
    "                predicted_df = pd.DataFrame(predicted_probabilities)\n",
    "                predicted_df.columns = ['exc']\n",
    "\n",
    "                #Concat DF with UTM coordinates \n",
    "                predicted_df = predicted_df.reset_index(drop=True)\n",
    "                prediction_locations = prediction_dataset[['lat','lng']].reset_index(drop=True)\n",
    "                predicted_df = pd.concat([predicted_df, prediction_locations],axis=1)\n",
    "                predicted_df['exc'] = predicted_df['exc']*100\n",
    "\n",
    "                predictions[m] = predicted_df.copy()\n",
    "\n",
    "                prediction_path = f'{best_base_path_predictions}/{pollutant}/predictions/{prefix}_prediction_month_{m}_stations_{train_mode}.csv'\n",
    "                predictions[m].to_csv(prediction_path)\n",
    "\n",
    "\n",
    "                print(f\"Predicting in grid datasets\")\n",
    "                predict_path = f'{best_base_path_predictions}/{pollutant}/predict/grid_month_{m}.csv'\n",
    "                prediction_df =  importer.import_df(predict_path, date_format=None)\n",
    "                prediction_dataset = prediction_df.copy()\n",
    "                prediction_dataset = prediction_dataset[training_columns]\n",
    "                predicts_grid[m] = prediction_df.copy()\n",
    "                if 'date' in list(prediction_dataset.columns):\n",
    "                    prediction_dataset = prediction_dataset.drop(['date'], axis=1)\n",
    "\n",
    "\n",
    "                prediction_dataset = prediction_dataset.reset_index(drop=True).dropna()\n",
    "                if model_to_use == 'lstm':\n",
    "                    predicted_probabilities = ai_model.predict(\n",
    "                        prediction_dataset.copy(), \n",
    "                        predict_options=model_options[model_to_use]['prediction_options']\n",
    "                    )\n",
    "                else:\n",
    "                    predicted_probabilities = ai_model.predict_probabilities(\n",
    "                        prediction_dataset.copy(), \n",
    "                        predict_options=model_options[model_to_use]['prediction_options']\n",
    "                    )\n",
    "                    predicted_probabilities = predicted_probabilities[:,1]\n",
    "\n",
    "                #Create a DF from predicted labels\n",
    "                predicted_df = pd.DataFrame(predicted_probabilities)\n",
    "                predicted_df.columns = ['exc']\n",
    "\n",
    "                #Concat DF with UTM coordinates \n",
    "                #reset index to avoid indexing problems\n",
    "                predicted_df = predicted_df.reset_index(drop=True)\n",
    "                prediction_locations = prediction_dataset[['lat','lng']].reset_index(drop=True)\n",
    "                predicted_df = pd.concat([predicted_df, prediction_locations],axis=1)\n",
    "                predicted_df['exc'] = predicted_df['exc']*100\n",
    "\n",
    "                predictions_grid[m] = predicted_df.copy()\n",
    "\n",
    "                prediction_path = f'{best_base_path_predictions}/{pollutant}/predictions/{prefix}_prediction_month_{m}_grid_{train_mode}.csv'\n",
    "                predictions_grid[m].to_csv(prediction_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45c2a7d3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (odc_env)",
   "language": "python",
   "name": "odc_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
