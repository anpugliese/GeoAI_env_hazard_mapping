{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ce75ed83",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-05 11:29:17.636193: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-10-05 11:29:17.637621: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-10-05 11:29:17.661734: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-10-05 11:29:17.662434: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-10-05 11:29:18.038974: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import math \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "import plotly.graph_objects as go\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import date, timedelta, datetime\n",
    "\n",
    "import interpolation_module as interp\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3acaf74c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_plot(df, x_col, y_col, name=\"Unnamed Plot\", additional_traces=[]):\n",
    "    if type(df) != list:\n",
    "        df = [df]\n",
    "        x_col = [x_col]\n",
    "        y_col = [y_col]\n",
    "        name = [name]\n",
    "    \n",
    "    fig = go.Figure()\n",
    "    for i, single_df in enumerate(df):\n",
    "        x = single_df[x_col[i]]\n",
    "        y = single_df[y_col[i]]\n",
    "        fig_name = name[i]\n",
    "        fig.add_trace(go.Scatter(x=x, y=y, mode='lines+markers',name=fig_name))\n",
    "    \n",
    "    if len(additional_traces) > 0:\n",
    "        for trace in additional_traces:\n",
    "            fig.add_trace(trace)\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "16571103",
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_df(path, date_format=\"%Y-%m-%dT%H:%M:%S\", date_column=\"date\"):\n",
    "    df = pd.read_csv(path, index_col = 0)\n",
    "    if date_format is not None and date_column is not None:\n",
    "        df[date_column] = pd.to_datetime(df[date_column],  format=date_format)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "489c30ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove Outliers function\n",
    "#This remove a point if it exceeds +-3 std deviations in a window of \"window\" observations \n",
    "# at column \"value_column\"\n",
    "def filter_outliers_by_sensor(input_df, window, sensor_list, value_column='value', sensor_column='sensor_id'):\n",
    "    filtered_df = pd.DataFrame()\n",
    "    for sensor in sensor_list:\n",
    "        df = input_df.copy()\n",
    "        df = df.loc[df[sensor_column] == sensor]\n",
    "        #iterate all the df\n",
    "        df['mean']= df[value_column].rolling(window, center=True, step=1, min_periods=1).mean()\n",
    "        df['std'] = df[value_column].rolling(window, center=True, step=1, min_periods=1).std()\n",
    "        #filter setup\n",
    "        df = df[(df[value_column] <= df['mean']+3*df['std']) & (df[value_column] >= df['mean']-3*df['std'])]\n",
    "        \n",
    "        filtered_df = pd.concat([filtered_df, df])\n",
    "    \n",
    "    del df\n",
    "    filtered_df = filtered_df.drop([\"mean\", \"std\"], axis=1)\n",
    "    return filtered_df\n",
    "\n",
    "\n",
    "def filter_outliers_global(input_df, sensor_list, value_column='value', sensor_column='sensor_id'):\n",
    "    filtered_df = pd.DataFrame()\n",
    "    for sensor in sensor_list:\n",
    "        df = input_df.copy()\n",
    "        df = df.loc[df[sensor_column] == sensor]\n",
    "        #iterate all the df\n",
    "        df['mean']= df[value_column].mean()\n",
    "        df['std'] = df[value_column].std()\n",
    "        #filter setup\n",
    "        df = df[(df[value_column] <= df['mean']+3*df['std']) & (df[value_column] >= df['mean']-3*df['std'])]\n",
    "        \n",
    "        filtered_df = pd.concat([filtered_df, df])\n",
    "    \n",
    "    del df\n",
    "    filtered_df = filtered_df.drop([\"mean\", \"std\"], axis=1)\n",
    "    return filtered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a099ec33",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Function to determine the section of the wind given the degree in which it is coming\n",
    "if 8 sectors:\n",
    "        North 337.5 - 22.5 -> sector 1\n",
    "        NE 22.5 - 67.5 -> sector 2\n",
    "        East 67.5 - 112.5 -> sector 3\n",
    "        SE 112.5 - 157.5 -> sector 4\n",
    "        South 157.5 - 202.5 -> sector 5\n",
    "        SW 202.5 - 247.5 -> sector 6\n",
    "        West 247.5 - 292.5 -> sector 7\n",
    "        NW 292.5 - 337.5 -> sector 8\n",
    "'''\n",
    "def wind_sectors(degree):\n",
    "    if degree >= 337.5 or degree < 22.5:\n",
    "        return 1\n",
    "    elif degree >= 22.5 and degree < 67.5:\n",
    "        return 2\n",
    "    elif degree >= 67.5 and degree < 112.5:\n",
    "        return 3\n",
    "    elif degree >= 112.5 and degree < 157.5:\n",
    "        return 4\n",
    "    elif degree >= 157.5 and degree < 202.5:\n",
    "        return 5\n",
    "    elif degree >= 202.5 and degree < 247.5:\n",
    "        return 6\n",
    "    elif degree >= 247.5 and degree < 292.5:\n",
    "        return 7\n",
    "    elif degree >= 292.5 and degree < 337.5:\n",
    "        return 8\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "22488791",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspiration and formula from here\n",
    "# https://stackoverflow.com/questions/1158909/average-of-two-angles-with-wrap-around\n",
    "def degree_average(degrees):\n",
    "    sin_sum = 0\n",
    "    cos_sum = 0\n",
    "    for deg in degrees:\n",
    "        #decompose the angles\n",
    "        deg_radians = math.radians(deg)\n",
    "        sin_sum += math.sin(deg_radians)\n",
    "        cos_sum += math.cos(deg_radians)    \n",
    "    \n",
    "    #use atan2 instead of atan\n",
    "    avg_radians = math.atan2(sin_sum, cos_sum)\n",
    "    avg_deg = math.degrees(avg_radians)\n",
    "    avg_deg = (avg_deg + 360) % 360 # to convert the angle between  0 - 360\n",
    "    return avg_deg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "46b869bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#global variables\n",
    "date_format = '%Y-%m-%d %H:%M:%S'\n",
    "save = False\n",
    "meteo_variables = [\n",
    "    \"temperature\",\n",
    "    \"precipitation\",\n",
    "    \"humidity\",\n",
    "    \"wind_velocity\",\n",
    "    \"wind_direction\",\n",
    "    \"global_radiation\",\n",
    "    \"hydrometric_level\"\n",
    "]\n",
    "\n",
    "#import initial meteo data\n",
    "temperature = import_df('../../data/milano_meteo_data/temperature.csv')\n",
    "precipitation = import_df('../../data/milano_meteo_data/precipitation.csv')\n",
    "humidity = import_df('../../data/milano_meteo_data/humidity.csv')\n",
    "wind_velocity = import_df('../../data/milano_meteo_data/wind_velocity.csv')\n",
    "wind_direction = import_df('../../data/milano_meteo_data/wind_direction.csv')\n",
    "global_radiation = import_df('../../data/milano_meteo_data/radiation.csv')\n",
    "hydrometric_level = import_df('../../data/milano_meteo_data/hydrometric_level.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "11fa86bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1214188/1630378873.py:15: FutureWarning: Dropping invalid columns in DataFrameGroupBy.agg is deprecated. In a future version, a TypeError will be raised. Before calling .agg, select only columns which should be valid for the function.\n",
      "  wind_direction_merge = wind_direction_merge.agg(degree_average)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merging\n",
      "Merge complete\n",
      "Removing outliers\n",
      "Data lost: 731\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>stationID</th>\n",
       "      <th>N</th>\n",
       "      <th>NE</th>\n",
       "      <th>E</th>\n",
       "      <th>SE</th>\n",
       "      <th>S</th>\n",
       "      <th>SW</th>\n",
       "      <th>W</th>\n",
       "      <th>NW</th>\n",
       "      <th>lat</th>\n",
       "      <th>lng</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2001-02-22</td>\n",
       "      <td>513</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.775000</td>\n",
       "      <td>3.633333</td>\n",
       "      <td>45.613692</td>\n",
       "      <td>9.508122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2001-02-23</td>\n",
       "      <td>513</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>3.033333</td>\n",
       "      <td>3.350000</td>\n",
       "      <td>45.613692</td>\n",
       "      <td>9.508122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2001-02-24</td>\n",
       "      <td>513</td>\n",
       "      <td>0.9</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.66</td>\n",
       "      <td>2.68</td>\n",
       "      <td>2.7</td>\n",
       "      <td>2.20</td>\n",
       "      <td>1.433333</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>45.613692</td>\n",
       "      <td>9.508122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2001-02-25</td>\n",
       "      <td>513</td>\n",
       "      <td>1.4</td>\n",
       "      <td>1.666667</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.80</td>\n",
       "      <td>1.8</td>\n",
       "      <td>1.10</td>\n",
       "      <td>1.350000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>45.613692</td>\n",
       "      <td>9.508122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2001-02-26</td>\n",
       "      <td>513</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.95</td>\n",
       "      <td>1.380000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>45.613692</td>\n",
       "      <td>9.508122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39249</th>\n",
       "      <td>2022-03-17</td>\n",
       "      <td>513</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.30</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>45.613692</td>\n",
       "      <td>9.508122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39250</th>\n",
       "      <td>2022-03-17</td>\n",
       "      <td>525</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>45.436109</td>\n",
       "      <td>9.097411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39251</th>\n",
       "      <td>2022-03-17</td>\n",
       "      <td>535</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>45.324517</td>\n",
       "      <td>9.134517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39252</th>\n",
       "      <td>2022-03-17</td>\n",
       "      <td>1547</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.40</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>45.517286</td>\n",
       "      <td>9.091610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39253</th>\n",
       "      <td>2022-03-17</td>\n",
       "      <td>1873</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.900000</td>\n",
       "      <td>45.394655</td>\n",
       "      <td>8.889706</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>39254 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             date  stationID    N        NE     E    SE    S    SW         W  \\\n",
       "0      2001-02-22        513  0.0  0.000000  0.00  0.00  0.0  0.00  2.775000   \n",
       "1      2001-02-23        513  2.3  0.700000  1.00  0.00  0.0  0.00  3.033333   \n",
       "2      2001-02-24        513  0.9  1.000000  2.66  2.68  2.7  2.20  1.433333   \n",
       "3      2001-02-25        513  1.4  1.666667  0.00  1.80  1.8  1.10  1.350000   \n",
       "4      2001-02-26        513  0.0  0.000000  0.00  0.00  0.0  1.95  1.380000   \n",
       "...           ...        ...  ...       ...   ...   ...  ...   ...       ...   \n",
       "39249  2022-03-17        513  0.0  0.000000  2.30  0.00  0.0  0.00  0.000000   \n",
       "39250  2022-03-17        525  0.0  0.700000  0.00  0.00  0.0  0.00  0.000000   \n",
       "39251  2022-03-17        535  0.7  0.000000  0.00  0.00  0.0  0.00  0.000000   \n",
       "39252  2022-03-17       1547  0.0  0.000000  1.40  0.00  0.0  0.00  0.000000   \n",
       "39253  2022-03-17       1873  0.0  0.000000  0.00  0.00  0.0  0.00  0.000000   \n",
       "\n",
       "             NW        lat       lng  \n",
       "0      3.633333  45.613692  9.508122  \n",
       "1      3.350000  45.613692  9.508122  \n",
       "2      1.500000  45.613692  9.508122  \n",
       "3      0.000000  45.613692  9.508122  \n",
       "4      1.000000  45.613692  9.508122  \n",
       "...         ...        ...       ...  \n",
       "39249  0.000000  45.613692  9.508122  \n",
       "39250  0.000000  45.436109  9.097411  \n",
       "39251  0.000000  45.324517  9.134517  \n",
       "39252  0.000000  45.517286  9.091610  \n",
       "39253  1.900000  45.394655  8.889706  \n",
       "\n",
       "[39254 rows x 12 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#fix columns and drop unnecessary\n",
    "wind_velocity_merge = wind_velocity.rename(columns = {'value':'wind_velocity', 'sensorID': 'sensor_wind_velocity', 'unit':'unit_wind_velocity'})\n",
    "wind_direction_merge = wind_direction.rename(columns = {'value':'wind_direction', 'sensorID': 'sensor_wind_direction', 'unit':'unit_wind_direction'})\n",
    "#round latitude and longitude to 6 decimal places -> cm level accuracy is enough\n",
    "wind_velocity_merge = wind_velocity_merge.round({'lat': 6, 'lng': 6})\n",
    "wind_direction_merge = wind_direction_merge.round({'lat': 6, 'lng': 6})\n",
    "#remove nodata and average duplicate date for the velocity dataset\n",
    "wind_velocity_merge = wind_velocity_merge.loc[wind_velocity_merge['wind_velocity'] != 999]\n",
    "wind_direction_merge = wind_direction_merge.loc[wind_direction_merge['wind_direction'] != 999]\n",
    "#use the mean for the velocities\n",
    "wind_velocity_merge = wind_velocity_merge.groupby(['sensor_wind_velocity','date','unit_wind_velocity','stationID','altitude','province','lat','lng']).mean(numeric_only=True)\n",
    "wind_velocity_merge = wind_velocity_merge.reset_index()\n",
    "#use a mean of degrees for the directions\n",
    "wind_direction_merge = wind_direction_merge.groupby(['sensor_wind_direction','date','unit_wind_direction','stationID','altitude','province','lat','lng'])\n",
    "wind_direction_merge = wind_direction_merge.agg(degree_average)\n",
    "wind_direction_merge = wind_direction_merge.reset_index()\n",
    "#merge wind direction and velocity in a single DF\n",
    "print(\"Merging\")\n",
    "wind_vel_dir = pd.merge(\n",
    "    wind_velocity_merge[\n",
    "        ['wind_velocity','sensor_wind_velocity','unit_wind_velocity','date','stationID','lat','lng']\n",
    "    ], \n",
    "    wind_direction_merge[\n",
    "        ['wind_direction','sensor_wind_direction','unit_wind_direction','date','stationID','lat','lng']\n",
    "    ], \n",
    "    how='inner', \n",
    "    on=['date', 'stationID','lat','lng']\n",
    ")\n",
    "print(\"Merge complete\")\n",
    "wind_vel_dir = wind_vel_dir.sort_values(by='date')\n",
    "#remove outliers\n",
    "print(\"Removing outliers\")\n",
    "stations_vel = list(wind_vel_dir['stationID'].unique())\n",
    "window = 24 #24 hours\n",
    "prev_len = len(wind_vel_dir)\n",
    "#wind_vel_dir = filter_outliers_global(\n",
    "wind_vel_dir = filter_outliers_by_sensor(\n",
    "    wind_vel_dir, \n",
    "    window,\n",
    "    stations_vel, \n",
    "    value_column='wind_velocity',\n",
    "    sensor_column='stationID'\n",
    ")\n",
    "print(f'Data lost: {(prev_len - len(wind_vel_dir))}')\n",
    "\n",
    "#determine the wind sector for each record\n",
    "wind_vel_dir['wind_sector'] = wind_vel_dir.apply(lambda row: wind_sectors(row['wind_direction']), axis=1)\n",
    "\n",
    "#pivot the wind to decompose the wind per day in each of the directions\n",
    "wind_vel_dir = wind_vel_dir.groupby([wind_vel_dir['date'].dt.date, 'stationID', 'wind_sector']).mean(numeric_only=True)\n",
    "wind_vel_dir = wind_vel_dir.reset_index()\n",
    "new_wind = wind_vel_dir[['date','stationID','wind_sector','wind_velocity','lat','lng']]\n",
    "pivoted_wind = new_wind.pivot(index=['date','stationID'], columns='wind_sector', values=['wind_velocity']).reset_index()\n",
    "pivoted_wind.columns = ['date', 'stationID', 'N', 'NE', 'E', 'SE', 'S', 'SW', 'W', 'NW']\n",
    "pivoted_wind = pivoted_wind.fillna(0)\n",
    "#round again the latitude and longitude to avoid duplicates due to dobule-precision issues\n",
    "wind_vel_dir = wind_vel_dir.round({'lat': 6, 'lng': 6})\n",
    "wind_stations = wind_vel_dir[['stationID', 'lat', 'lng']].drop_duplicates()\n",
    "#merge the station location with the pivoted wind\n",
    "wind = pd.merge(pivoted_wind, wind_stations, how='left', on='stationID')\n",
    "wind\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "41e7d42b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For WIND\n",
      "stations 14\n",
      "unique dates 7531\n",
      "date range: 2001-02-22 - 2022-03-17\n",
      "total days 7693\n",
      "data days 7531\n",
      "days lost 162 --> 2.105810477057065%\n"
     ]
    }
   ],
   "source": [
    "print(\"For WIND\")\n",
    "un_stations = len(wind.stationID.unique())\n",
    "print(f'stations {un_stations}')\n",
    "un_dates = len(wind.date.unique())\n",
    "print(f'unique dates {un_dates}')\n",
    "initial_day = list(wind.date.unique())[0]\n",
    "last_day = list(wind.date.unique())[-1]\n",
    "t_days = (last_day - initial_day).days\n",
    "lost_days = t_days - un_dates\n",
    "print(f'date range: {initial_day} - {last_day}')\n",
    "print(\"total days\", t_days)\n",
    "print(\"data days\", un_dates)\n",
    "print(f\"days lost {lost_days} --> {(lost_days/t_days)*100}%\")\n",
    "#th_size = t_days * un_stations\n",
    "#print(\"theoretical data size\", th_size)\n",
    "#data_lost = len(wind)\n",
    "#data_lost = (th_size - data_size)\n",
    "#print(\"real data size\", data_size)\n",
    "#print(f\"data lost {data_lost} --> {(data_lost/th_size)*100}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7689104d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if save:\n",
    "    wind.to_csv('../../data/milano_meteo_data/wind_daily.csv')\n",
    "else:\n",
    "    wind = pd.read_csv('../../data/milano_meteo_data/wind_daily.csv', index_col = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "86cd920f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'---------------------------------------------------------------------------------------------------'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"---------------------------------------------------------------------------------------------------\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4699029d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_meteo_dataset(meteo_df, name):\n",
    "    print(f\"start {name}\")\n",
    "    #Prepare the meteo data\n",
    "    df = meteo_df.copy()\n",
    "    #fix column names\n",
    "    df = df.rename(columns = {'value': f'{name}', 'sensorID': f'sensor_{name}', 'unit': f'unit_{name}'})\n",
    "    #take only necessary columns\n",
    "    df = df[['date','stationID','lat','lng', name, f'sensor_{name}']]\n",
    "    df = df.round({'lat': 6, 'lng': 6})\n",
    "    #remove nodata\n",
    "    df = df.loc[df[name] != 999]\n",
    "    #special clean for temperature\n",
    "    if name == 'temperature':\n",
    "        df = df.loc[df[name] != -29.9]\n",
    "        df = df.loc[df[name] != -30]\n",
    "    #mean for the values of the same datetime\n",
    "    df = df.groupby(['date','stationID','lat','lng', f'sensor_{name}']).mean()\n",
    "    df = df.reset_index()\n",
    "    #remove outliers\n",
    "    window = 6*24*7 #each 10 minutes -> 6(to hours)*24(to days)*7(to 1 week)\n",
    "    stations_list = list(df['stationID'].unique())\n",
    "    #df = filter_outliers_by_sensor(\n",
    "    df = filter_outliers_global(\n",
    "        df, \n",
    "        #window, \n",
    "        stations_list, \n",
    "        value_column=name,\n",
    "        sensor_column='stationID'\n",
    "    )\n",
    "    #sort by day after outlier removal\n",
    "    df = df.sort_values(by='date')\n",
    "    #set the date column as a datetime\n",
    "    df['date'] = pd.to_datetime(df['date'], format = '%Y-%m-%d')\n",
    "    print(f\"end {name}\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7321b396",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_meteo_df(main_df, df_to_merge, name_to_merge, operation=\"mean\"):\n",
    "    if main_df is None:\n",
    "        return df_to_merge\n",
    "    else:\n",
    "        df = main_df.copy()\n",
    "        df_columns = list(df.columns)\n",
    "        df = pd.merge(df, df_to_merge, how='outer', on=['date','stationID','lat','lng'])\n",
    "    \n",
    "    select = df_columns\n",
    "    select.append(name_to_merge)\n",
    "    select.append(f'sensor_{name_to_merge}')     \n",
    "    df = df[select]\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e199ce08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def meteo_group_by_day(meteo_df, name, operation='mean'):\n",
    "    #group by date and with the operation especified\n",
    "    df_grouped = meteo_df.copy()\n",
    "    df_grouped['date'] = pd.to_datetime(df_grouped['date']).dt.date\n",
    "    df_grouped = df_grouped = df_grouped.groupby(['date','stationID',f'sensor_{name}','lat','lng'])\n",
    "    if operation == \"mean\":\n",
    "        df_grouped = df_grouped.mean()\n",
    "    elif operation == \"sum\":\n",
    "        df_grouped = df_grouped.sum()\n",
    "    else:\n",
    "        df_grouped = df_grouped.count()\n",
    "        \n",
    "    df_grouped = df_grouped.reset_index()\n",
    "    return df_grouped\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "030bb390",
   "metadata": {},
   "outputs": [],
   "source": [
    "#clean the meteo datasets\n",
    "'''\n",
    "temperature = 'variables/temperature.csv'\n",
    "precipitation.to_csv('variables/precipitation.csv')\n",
    "humidity.to_csv('variables/humidity.csv')\n",
    "wind_velocity.to_csv('variables/wind_velocity.csv')\n",
    "global_radiation.to_csv('variables/global_radiation.csv')\n",
    "hydrometric_level.to_csv('variables/hydrometric_level.csv')\n",
    "wind_direction.to_csv('variables/wind_direction.csv')\n",
    "'''\n",
    "meteo_names = ['temperature', 'precipitation', 'humidity', 'global_radiation', 'hydrometric_level']\n",
    "meteo_df = [temperature, precipitation, humidity, global_radiation, hydrometric_level]\n",
    "meteo_data = {\n",
    "    \"temperature\": {\"name\": \"temperature\", \"df\": temperature, \"clean_df\": None, \"operation\": \"mean\"},\n",
    "    \"precipitation\": {\"name\": \"precipitation\", \"df\": precipitation, \"clean_df\": None, \"operation\": \"sum\"},\n",
    "    \"humidity\": {\"name\": \"humidity\", \"df\": humidity, \"clean_df\": None, \"operation\": \"mean\"},\n",
    "    \"global_radiation\": {\"name\": \"global_radiation\", \"df\": global_radiation, \"clean_df\": None, \"operation\": \"mean\"},\n",
    "    \"hydrometric_level\": {\"name\": \"hydrometric_level\", \"df\": hydrometric_level, \"clean_df\": None, \"operation\": \"mean\"}\n",
    "}\n",
    "\n",
    "for meteo_name in meteo_names:\n",
    "    meteo_data[meteo_name][\"clean_df\"] = clean_meteo_dataset(meteo_data[meteo_name][\"df\"], meteo_name)\n",
    "    meteo_data[meteo_name][\"clean_df\"] = meteo_group_by_day(meteo_data[meteo_name]['clean_df'], meteo_name, meteo_data[meteo_name]['operation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cd6a2be",
   "metadata": {},
   "outputs": [],
   "source": [
    "ls = [\n",
    "    list(meteo_data['temperature']['clean_df'].stationID.unique()),\n",
    "    list(meteo_data['precipitation']['clean_df'].stationID.unique()),\n",
    "    list(meteo_data['humidity']['clean_df'].stationID.unique()),\n",
    "    list(meteo_data['global_radiation']['clean_df'].stationID.unique()),\n",
    "    list(meteo_data['hydrometric_level']['clean_df'].stationID.unique()),\n",
    "]\n",
    "l = list(wind_avg.stationID.unique())\n",
    "agg = l.copy()\n",
    "for j in range(5):     \n",
    "    agg = agg + ls[j]\n",
    "            \n",
    "agg = list(dict.fromkeys(agg))\n",
    "print(len(agg))\n",
    "print(agg)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ea2bc31",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"For Temperature\")\n",
    "un_stations = len(meteo_data['temperature']['clean_df'].stationID.unique())\n",
    "print(un_stations)\n",
    "un_dates = len(meteo_data['temperature']['clean_df'].date.unique())\n",
    "print(un_dates)\n",
    "t_days = ((2023 - 2016) * 365) + 2 #leap years\n",
    "print(\"total days\", t_days)\n",
    "print(\"data days\", un_dates)\n",
    "print(\"days lost\", t_days - un_dates)\n",
    "th_size = t_days * un_stations\n",
    "print(\"theoretical data size\", th_size)\n",
    "data_size = len(wind_max)\n",
    "print(\"real data size\", data_size)\n",
    "print(\"data lost\", (th_size - data_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d89a3d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_temp = meteo_data['temperature']['clean_df'].copy()\n",
    "df_temp['date'] = pd.to_datetime(df_temp['date'])\n",
    "\n",
    "months = [1,2,3,4,5,6,7,8,9,10,11,12]\n",
    "meteo_months = {}\n",
    "for month in months:\n",
    "    meteo_months[month] = df_temp.loc[df_temp['date'].dt.month == month]\n",
    "    meteo_months[month] = meteo_months[month].sort_values('date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7a2b907",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_plot(meteo_months[7], 'date', 'temperature', name=\"January temperature\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6881daa9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "570ce1ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "meteo_all = None\n",
    "for meteo in meteo_names:\n",
    "    df_to_merge = meteo_data[meteo]['clean_df']\n",
    "    operation = meteo_data[meteo]['operation']\n",
    "    print(f\"merging {meteo}\")\n",
    "    meteo_all = merge_meteo_df(meteo_all, df_to_merge, meteo, operation=operation)\n",
    "    print(f\"merged {meteo}\")   \n",
    "    \n",
    "#merge wind\n",
    "print(f\"merging wind\")\n",
    "meteo_all = pd.merge(meteo_all, wind_avg, how='outer', on=['date','stationID','lat','lng'])\n",
    "meteo_all = pd.merge(meteo_all, wind_max, how='outer', on=['date','stationID','lat','lng'])\n",
    "print(f\"merged wind\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5808ae2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "meteo_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cec7cee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove the columns of the sensor ids\n",
    "meteo_all = meteo_all[['date','lat','stationID','lng','temperature','precipitation','humidity','global_radiation','hydrometric_level','N_avg','NE_avg','E_avg','SE_avg','S_avg','SW_avg','W_avg','NW_avg','N_max','NE_max','E_max','SE_max','S_max','SW_max','W_max','NW_max']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20cff9f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#meteo_copy = meteo_all.copy()\n",
    "#meteo_all = meteo_copy.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a4e7ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "meteo_outlier_names = ['temperature', 'humidity', 'global_radiation', 'hydrometric_level']\n",
    "#Post-merge global outlier cleaning - does not include precipitation as it is a sum, not a mean\n",
    "for meteo_name in meteo_outlier_names:\n",
    "    column_avg = meteo_all[meteo_name].mean()\n",
    "    column_std = meteo_all[meteo_name].std()\n",
    "    out_low = column_avg - (3*column_std)\n",
    "    out_hi = column_avg + (3*column_std)\n",
    "    print(meteo_name, out_low, out_hi)\n",
    "    meteo_all[meteo_name] = np.where(\n",
    "        ((meteo_all[meteo_name] > out_hi) | (meteo_all[meteo_name] < out_low)), \n",
    "        np.nan, \n",
    "        meteo_all[meteo_name]\n",
    "    )\n",
    "meteo_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18962d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "meteo_all.to_csv('../../data/milano_meteo_data/meteo_data_merged_daily.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0830891b",
   "metadata": {},
   "outputs": [],
   "source": [
    "un_stations = len(meteo_all.stationID.unique())\n",
    "print(un_stations)\n",
    "un_dates = len(meteo_all.date.unique())\n",
    "print(un_dates)\n",
    "t_days = ((2023 - 2016) * 365) + 2 #leap years\n",
    "print(\"total days\", t_days)\n",
    "print(\"data days\", un_dates)\n",
    "print(\"days lost\", t_days - un_dates)\n",
    "th_size = t_days * un_stations\n",
    "print(\"theoretical data size\", th_size)\n",
    "data_size = len(meteo_all)\n",
    "print(\"real data size\", data_size)\n",
    "print(\"data lost\", (th_size - data_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0b74dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"---------------------------------------------------------------------------------------------------\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "266e566e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Interpolate where data is missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53f75095",
   "metadata": {},
   "outputs": [],
   "source": [
    "meteo_all = pd.read_csv('../../data/milano_meteo_data/meteo_data_merged_daily.csv', index_col = 0)\n",
    "meteo_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ae52ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "inter_meteo = meteo_all.copy()\n",
    "#meteo_all = meteo_all_copy.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "332c05b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "inter_meteo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "641df440",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#start_date = date(2016,1,1)\n",
    "#end_date = date(2022,12,31)\n",
    "variable_list = ['temperature', 'precipitation', 'humidity', 'hydrometric_level', 'global_radiation',\n",
    "                'N_avg', 'NE_avg', 'E_avg', 'SE_avg', 'S_avg', 'SW_avg', 'W_avg', 'NW_avg',\n",
    "                'N_max', 'NE_max', 'E_max', 'SE_max', 'S_max', 'SW_max', 'W_max', 'NW_max']\n",
    "\n",
    "date_range = [date(2016,1,1), date(2022,12,31)]\n",
    "for var_name in variable_list:\n",
    "    print(f'-----------------------------{var_name}---------------------------------')\n",
    "    interp.single_NN(inter_meteo,var_name, date_range)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa7cad15",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fix interpolation negative values\n",
    "inter_meteo.loc[inter_meteo['precipitation']<0,'precipitation']=0\n",
    "inter_meteo.loc[inter_meteo['global_radiation']<0,'global_radiation']=0\n",
    "inter_meteo.loc[inter_meteo['N_avg']<0,'N_avg']=0\n",
    "inter_meteo.loc[inter_meteo['NE_avg']<0,'NE_avg']=0\n",
    "inter_meteo.loc[inter_meteo['E_avg']<0,'E_avg']=0\n",
    "inter_meteo.loc[inter_meteo['SE_avg']<0,'SE_avg']=0\n",
    "inter_meteo.loc[inter_meteo['S_avg']<0,'S_avg']=0\n",
    "inter_meteo.loc[inter_meteo['SW_avg']<0,'SW_avg']=0\n",
    "inter_meteo.loc[inter_meteo['W_avg']<0,'W_avg']=0\n",
    "inter_meteo.loc[inter_meteo['NW_avg']<0,'NW_avg']=0\n",
    "inter_meteo.loc[inter_meteo['N_max']<0,'N_max']=0\n",
    "inter_meteo.loc[inter_meteo['NE_max']<0,'NE_max']=0\n",
    "inter_meteo.loc[inter_meteo['E_max']<0,'E_max']=0\n",
    "inter_meteo.loc[inter_meteo['SE_max']<0,'SE_max']=0\n",
    "inter_meteo.loc[inter_meteo['S_max']<0,'S_max']=0\n",
    "inter_meteo.loc[inter_meteo['SW_max']<0,'SW_max']=0\n",
    "inter_meteo.loc[inter_meteo['W_max']<0,'W_max']=0\n",
    "inter_meteo.loc[inter_meteo['NW_max']<0,'NW_max']=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c5f19fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#inter_meteo.describe()\n",
    "inter_meteo.to_csv('../../data/milano_meteo_data/meteo_data_merged_daily_interpolated.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20782fdd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (odc_env)",
   "language": "python",
   "name": "odc_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
